{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Assignment 1\n",
    "\n",
    "Submitted by: Prateek Singh( 15BCE1091) \n",
    "<br>\n",
    "<br>\n",
    "The dataset used in this experiment is ionosphere dataset from UCI Machine learning repository. \n",
    "\n",
    "## Dataset Description\n",
    "This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. \n",
    "The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \n",
    "\"Bad\" returns are those that do not; their signals pass through the ionosphere. \n",
    "\n",
    "The different regularization algos applied in this experiment are:\n",
    "<li>L1 (Lasso) Regularization</li>\n",
    "<li>L2 (Ridge) Regularizatoin</li>\n",
    "<li>Elastic Net</li>\n",
    "<li>LARS-Lasso</li>\n",
    "<li>AIC</li>\n",
    "<li>BIC</li>\n",
    "<li>Orthogonal Matching Pursuit</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, OrthogonalMatchingPursuit, LassoLars, LassoLarsIC, ElasticNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1        2        3        4        5        6        7        8   \\\n",
       "0   1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1   1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2   1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3   1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4   1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "\n",
       "        9  ...       25       26       27       28       29       30       31  \\\n",
       "0  0.03760 ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267 -0.54487   \n",
       "1 -0.04549 ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626 -0.06288   \n",
       "2  0.01198 ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436 -0.24180   \n",
       "3  0.00000 ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682  1.00000   \n",
       "4 -0.16399 ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707 -0.59573   \n",
       "\n",
       "        32       33  34  \n",
       "0  0.18641 -0.45300   g  \n",
       "1 -0.13738 -0.02447   b  \n",
       "2  0.56045 -0.38238   g  \n",
       "3 -0.32382  1.00000   b  \n",
       "4 -0.04608 -0.65697   g  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Dataset/ionosphere.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's check to total number of classes the dataset is divided into, although in this case we are already given that in the description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points for class: \n",
      "\n",
      "b  : 126\n",
      "g  : 225\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(data.iloc[:, 34].unique())\n",
    "\n",
    "print(\"Number of data points for class: \\n\")\n",
    "\n",
    "for cls in classes:\n",
    "    num_samples = len(data.loc[data.iloc[:, 34] == cls])\n",
    "    print(cls, \" :\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we try to vizualize the dataset, however since the dataset is 33 dimensional we have to reduce the dimensions in order to be able to effectively project the dataset and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzbUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "transformed_data = pd.DataFrame(pca.fit_transform(data.iloc[:, :34]))\n",
    "Y = pd.Series.to_frame(data.iloc[:, 34])\n",
    "\n",
    "plt.scatter(transformed_data.loc[list(Y.loc[Y[34] == 'b'].index)][0], transformed_data.loc[list(Y.loc[Y[34] == 'b'].index)][1], label='bad', c='red')\n",
    "plt.scatter(transformed_data.loc[list(Y.loc[Y[34] == 'g'].index)][0], transformed_data.loc[list(Y.loc[Y[34] == 'g'].index)][1], label='good', c='green')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use the train_test_split function by sklearn to split the dataset between training and testing data such that 10% of the data is for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  315\n",
      "Size of test set:  36\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data.iloc[:, :34], \n",
    "                                                     data.iloc[:, 34], \n",
    "                                                     test_size=0.1, \n",
    "                                                     random_state=42)\n",
    "\n",
    "Y_train = pd.Series.to_frame(Y_train)\n",
    "Y_test = pd.Series.to_frame(Y_test)\n",
    "\n",
    "print(\"Size of training set: \", len(X_train.axes[0]))\n",
    "print(\"Size of test set: \", len(X_test.axes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90932</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.86528</td>\n",
       "      <td>0.16888</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.16598</td>\n",
       "      <td>0.55187</td>\n",
       "      <td>0.68154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24856</td>\n",
       "      <td>0.56527</td>\n",
       "      <td>0.18626</td>\n",
       "      <td>0.56605</td>\n",
       "      <td>0.12635</td>\n",
       "      <td>0.56101</td>\n",
       "      <td>0.06927</td>\n",
       "      <td>0.55061</td>\n",
       "      <td>0.12137</td>\n",
       "      <td>0.67739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.14236</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.16256</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.23656</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.07514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.47643</td>\n",
       "      <td>0.98820</td>\n",
       "      <td>-0.49687</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.75820</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.75761</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.84437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00641</td>\n",
       "      <td>-0.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.01923</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.61538</td>\n",
       "      <td>-0.51282</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.93658</td>\n",
       "      <td>0.35107</td>\n",
       "      <td>0.75254</td>\n",
       "      <td>0.65640</td>\n",
       "      <td>0.45571</td>\n",
       "      <td>0.88576</td>\n",
       "      <td>0.15323</td>\n",
       "      <td>0.95776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36799</td>\n",
       "      <td>-0.84951</td>\n",
       "      <td>-0.04578</td>\n",
       "      <td>-0.91221</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>-0.85762</td>\n",
       "      <td>0.54827</td>\n",
       "      <td>-0.69613</td>\n",
       "      <td>0.74828</td>\n",
       "      <td>-0.44173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.23395</td>\n",
       "      <td>0.91404</td>\n",
       "      <td>0.52013</td>\n",
       "      <td>0.78020</td>\n",
       "      <td>0.72144</td>\n",
       "      <td>0.47660</td>\n",
       "      <td>0.84222</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.70838</td>\n",
       "      <td>0.03591</td>\n",
       "      <td>-0.71731</td>\n",
       "      <td>-0.11943</td>\n",
       "      <td>-0.64962</td>\n",
       "      <td>-0.28183</td>\n",
       "      <td>-0.51251</td>\n",
       "      <td>-0.44505</td>\n",
       "      <td>-0.37432</td>\n",
       "      <td>-0.53319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1        2        3        4        5        6        7        8   \\\n",
       "42    1   0  0.90932  0.08791  0.86528  0.16888  1.00000  0.16598  0.55187   \n",
       "30    1   0  1.00000 -0.14236  1.00000 -0.16256  1.00000 -0.23656  1.00000   \n",
       "208   1   0 -0.00641 -0.50000  0.00000  0.00000 -0.01923  1.00000  0.00000   \n",
       "310   1   0  0.93658  0.35107  0.75254  0.65640  0.45571  0.88576  0.15323   \n",
       "113   1   0  1.00000  0.23395  0.91404  0.52013  0.78020  0.72144  0.47660   \n",
       "\n",
       "          9    ...          24       25       26       27       28       29  \\\n",
       "42   0.68154   ...     0.24856  0.56527  0.18626  0.56605  0.12635  0.56101   \n",
       "30  -0.07514   ...     1.00000 -0.47643  0.98820 -0.49687  1.00000 -0.75820   \n",
       "208  0.00000   ...     0.00000  0.00000 -0.61538 -0.51282  0.00000  0.00000   \n",
       "310  0.95776   ...    -0.36799 -0.84951 -0.04578 -0.91221  0.27330 -0.85762   \n",
       "113  0.84222   ...    -0.70838  0.03591 -0.71731 -0.11943 -0.64962 -0.28183   \n",
       "\n",
       "          30       31       32       33  \n",
       "42   0.06927  0.55061  0.12137  0.67739  \n",
       "30   1.00000 -0.75761  1.00000 -0.84437  \n",
       "208  0.00000  0.00000  0.00000  0.00000  \n",
       "310  0.54827 -0.69613  0.74828 -0.44173  \n",
       "113 -0.51251 -0.44505 -0.37432 -0.53319  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     34\n",
       "42    1\n",
       "30    1\n",
       "208   0\n",
       "310   1\n",
       "113   1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(Y_train)\n",
    "le.classes_\n",
    "Y_train = pd.DataFrame(le.transform(Y_train), index=Y_train.index, columns=Y_train.columns)\n",
    "Y_test = pd.DataFrame(le.transform(Y_test), index=Y_test.index, columns=Y_test.columns)\n",
    "Y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use a vanilla logistic regression without any regularization with which we achieve and accuracy of 92.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.926984126984127\n",
      "Test Score:  0.8333333333333334 \n",
      "\n",
      "[[ 9  6]\n",
      " [ 0 21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_l1 = LogisticRegression(solver='saga', max_iter=10000, multi_class='multinomial')\n",
    "model_l1.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train Score: \", model_l1.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_l1.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_l1.predict(X_test)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.9142857142857143\n",
      "Test Score:  0.8333333333333334 \n",
      "\n",
      "[[ 9  6]\n",
      " [ 0 21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_l1 = LogisticRegression(penalty='l1', solver='saga', max_iter=10000, multi_class='multinomial')\n",
    "model_l1.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train Score: \", model_l1.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_l1.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_l1.predict(X_test)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.926984126984127\n",
      "Test Score:  0.8333333333333334 \n",
      "\n",
      "[[ 9  6]\n",
      " [ 0 21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_l2 = LogisticRegression(penalty='l2', solver='saga', max_iter=10000, multi_class='multinomial')\n",
    "model_l2.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train Score: \", model_l2.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_l2.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_l2.predict(X_test)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.0\n",
      "Test Score:  -0.01700291545189514 \n",
      "\n",
      "[[15  0]\n",
      " [21  0]]\n"
     ]
    }
   ],
   "source": [
    "model_v3 = ElasticNet(random_state=0)\n",
    "model_v3.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train Score: \", model_v3.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_v3.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_v3.predict(X_test)\n",
    "Y_pred = Y_pred.astype(int)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.20716869380731406\n",
      "Test Score:  0.18172349343699712 \n",
      "\n",
      "[[15  0]\n",
      " [21  0]]\n"
     ]
    }
   ],
   "source": [
    "model_v4 = LassoLars(alpha=0.01)\n",
    "model_v4.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train Score: \", model_v4.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_v4.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_v4.predict(X_test)\n",
    "Y_pred = Y_pred.astype(int)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AIC regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.53682743360097\n",
      "Test Score:  0.4201768645178431 \n",
      "\n",
      "[[15  0]\n",
      " [21  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X_train, Y_train)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "print(\"Train Score: \", model_aic.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_aic.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_aic.predict(X_test)\n",
    "Y_pred = Y_pred.astype(int)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BIC regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.5256014573187816\n",
      "Test Score:  0.42136955872651527 \n",
      "\n",
      "[[15  0]\n",
      " [21  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(X_train, Y_train)\n",
    "alpha_bic_ = model_aic.alpha_\n",
    "\n",
    "print(\"Train Score: \", model_bic.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_bic.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_bic.predict(X_test)\n",
    "Y_pred = Y_pred.astype(int)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Orthogonal Matching Pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.6376814190887808\n",
      "Test Score:  0.35222528616415316 \n",
      "\n",
      "[[14  1]\n",
      " [19  2]]\n"
     ]
    }
   ],
   "source": [
    "model_omp = OrthogonalMatchingPursuit(n_nonzero_coefs=33)\n",
    "model_omp.fit(X_train, Y_train)\n",
    "coef = model_omp.coef_\n",
    "\n",
    "print(\"Train Score: \", model_omp.score(X_train, Y_train))\n",
    "print(\"Test Score: \", model_omp.score(X_test, Y_test), '\\n')\n",
    "\n",
    "Y_pred = model_omp.predict(X_test)\n",
    "Y_pred = Y_pred.astype(int)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
